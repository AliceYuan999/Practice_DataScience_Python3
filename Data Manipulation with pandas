3. Data Manipulation with pandas
Content:
1. Chapter 1: Transforming DataFrames
    1. introducing DataFrames
    2. Sorting and subsetting
    3. New columns
2. Aggregating DataFrames
    1. Summary statistics
    2. Counting 
    3. Grouped summary statistics
    4. Pivot tables
3. Slicing and indexing DataFrames
    1. Explicit indexes
    2. Slicing and subsetting with .loc[] and .iloc[]
    3. Working with pivot tables
4. Creating and visualising DataFrames
    1. Visualising your data
    2. Missing values
    3. Creating DataFrames
    4. Reading and writing CSVs 

Chapter 1 : DataFrames
    1. Sorting and subsetting 
        * .head() returns the first few rows (the “head” of the DataFrame).
        * .info() shows information on each of the columns, such as the data type and number of missing values.
        * .shape returns the number of rows and columns of the DataFrame.
        * .describe() calculates a few summary statistics for each column.
        * .values: A two-dimensional NumPy array of values.
        * .columns: An index of columns: the column names.
        * .index: An index for the rows: either row numbers or row names.
	
	
        * .sort_values() can be used to sort rows by passing a column name.
            * One column : df.sort_values(“breed”, ascending = False)
            * Multiple columns : df.sort_values([“breed”, “weignt_kg”], ascending = [True, False])	
            * To select subset of DataFrame columns, follow the DataFrame name with square brackets containing the name of the columns: df[“col_a”]
            * To subset multiple columns , pass a list of column names to the square brackets  df[["col_a", "col_b”]]

        * Subsetting DataFrames on a single condition: Rows are often subset using a logical condition to perform filtering. To create a condition, use square brackets to substitute the column to filter on, then compare it to a value using a comparison operator.
            * dogs[dogs["height_cm"] > 60]
            * dogs[dogs["color"] == "tan"]

        * You can combine multiple conditions s=using the comparison operation & (and) and | (or).
            * dogs[(dogs["height_cm"] > 60) & (dogs["color"] == "tan")]
            * mountains[(mountains[“height_m”] > 8600) | (mountains[“ascent”] > “1956-01-01”)]

        * .isin() method is used to subset DataFrame rows containing one of multiple values of a categorical variable
            * south_mid_atlantic = homelessness[homelessness["region"].isin(["South Atlantic", "Mid-Atlantic"])]
            * colors = ["brown", "black", "tan”]; condition = dogs["color"].isin(colors); dogs[condition]

    2. Creating new columns
        * Adding new DataFrame columns
            * mountains[“hight_km”] = mountains[“hight_m”]/1000

Chapter 2: Aggregating Data
    1. Summary statistics
        * .mean() method compute the “center” of a DataFrame column. Like this: sales[‘profit’].mean()
        * .min() method returns the minimum values.
        * .max() method returns the maximum values.
        * .median() method returns the median values.
        * .mode() method returns the mode.
        * .std() computes the standard deviation 
        * .var() computes variance.
        * .sum() computes column sums.
        * .quantile() calculates quantiles.

        * .agg() DataFrame method allows you to compute custom summary statistics. Like this: salse[‘profit’].agg(function). For instance: salse[‘profit’].agg(iqr)        # "IQR" is short for inter-quartile range, which is the 75th percentile minus the 25th percentile. 
            * To compute the same statistic on multiple columns, subset multiple columns using double square brackets before the .agg() call.
                * salse[[‘sold’, ‘profit’]].agg(pct95)
            * To compute different summary statistics at the same time.
                * sales[‘profit’].agg([pct70, pct95])

        * .cumsum() method performs a cumulative sum on a column. It returns one number for every row of the DataFrame, where the first number is the first value, the second is the sum of the first and the second, and so on. Like this: sales[‘profit’].cumsum()
        * .cummin() for the cumulative minimum.
        * .cummax() for the cumulative maximum.
        * .cumprod() for the cumulative product.

        * .drop_duplicates() method drops rows containing duplicated information from a DataFrame. The column used to identify duplicates is specified in the stebset arg. 
            * Like this: sales.drop_duplicates(subset = ‘color’)
        * To drop rows based on multiple columns, pass subset a list of column names. This will drop rows if there is already a row with the same values in the specified columns. 
            * Like this: sales.drop_duplicates(subset = [‘type’, color])


    2. Counting 
        *  .value_counts() method returns the count of unique values in a column.
            * sales[‘type’].value_counts()
        * Setting the argument sort=True will order the counts from highest to lowest.
            * sales[‘type’].value_counts(sort = True)
        * The normalize argument turns the counts into proportions of the total.
            * sales[‘type’].value_counts(normalize = True)

    3. Grouped summary statistics
        * .groupby() takes the column name to group by as an argument. The subset the column to summarise with square brackets and call the summary statistic method. 
            * sales.groupby(‘type’)[‘sold’].max
        * Like ungrouped statistics, you can use .agg() to calculate multiple summary statistics on each group. To do this, pass .agg() a list of functions to use.
            * sales.groupby(‘type’)[‘sold’].agg([max, sum])
        * The group by multiple columns, pass a list of column names to .groupby(). Then subset the values of interest and call a summary statistic method.
            * sales.groupby([‘type’, ‘color’])[‘sole’].sum()
        * You can also group by multiple columns and aggregate by multiple columns. To do this, subset the desired columns using double square brackets after .groupby()
            * Sales.groupby([‘type’, ‘color’])[[‘sold’, ‘profit’]].sum()

        * .pivot_table() method allows you to calculate summary statistics on different groups, similarly to .groupby(). Pass the column to group by to the index agreement and the column to summarise to values. By default, .pivot_table() calculates the mean for each group.
            * sales.pivot_table(values=‘sold’, index=‘type’)
        *  By default, .pivot_table() calculates the mean for each group, but you can also pass an alternative function to the aggfunc argument
            * sales.pivot_table(values=‘sold’, index=‘type’, aggfunc=np.median)
        * To calculate multiple summary statistics, pass aggfunc a list of functions to use.
            * sales.pivot_table(values=‘sold’, index=‘type’, aggfunc=[np.mean, np.median])

        * Pivoting on two variables: Pass the first column to group by to the index argument, the second column to group by to columns, and the column to summarise to values. NaN values fill the spaces where data is missing.
            * sales.pivot_table(values=‘sold’, index=‘type’, columns= ‘color’)
        * The NaN  values in the pivot table can be replaced with a values specified by the fill_value argument.
            * sales.pivot_table(values=‘sold’, index=‘type’, columns=‘color’, fill_values=0)
        * Setting the argument margins=True will create a new column and row containing the mean of the values in that column or row (not including missing values.)
            * Sales.pivot_table(values=‘sold’, index=‘’type, columns= ‘color’, margins = True)

Chapter 3 : Slicing and Indexing Data
    1. Indexes and subsetting using indexes
        * .set_index() allows you to subset rows with less code.
            * sales.set_index(‘type’).head
            * temperatures_ind = temperatures.set_index('city')
        * To reset the index to sequential integers, call the .reset_index() method
            * sales.reset_index().head
            * print(temperatures_ind.reset_index())
        * To drop the index column form the DataFrame, pass drop=True to .reset_index()
            * sales.reset_index(drop=True).head()
            * print(temperatures_ind.reset_index(drop=True))

        * You can include multiple columns in the DataFrame index by tasing a list of columns to .set_index(). These are called multi-level indexes, or hierarchical indexes.
            * sales.set_index([‘type’, ‘color’]).head()

        * To subset a row with a particular index, call .loc[] on the DataFrame and pass it the index to subset.
            * sales.loc[‘shoes’]
        * To subset multiple indexes, pass .loc[] a list of indexes 
            * sales.loc[[‘shorts’, ‘ shoes’]]
        * To subset inners level, pass .loc[] a list of tuples. Each tuple should specify which outer and inner levels are required for the row to be included in the subset
            * sales.loc[[(‘shirt’, ‘black’), (‘shoes, ‘white’’)]]

        * .sort_index() sorts all index levels from outer to inner in ascending order.
            * sales.sort_index()
        * To modify the sorting behaviour, pass the levels argument a list of levels in the order that they should be sorted. Then pass ascending a list of Booleans indicate the ordering direction for each level
            * sales.sort_index(level=[‘color’, ‘type’], ascending = [True, False])

    2. Subsetting using slicing：
        * Similar to lists, DataFrame rows and columns can be sliced. To slice a DataFrame, the index must first be sorted busing .sort_index(). To slice rows at the outer index level, call .loc[] on the DataFrame and pass it the first and last index values separated by a :. Like lists, the final value is included in the slice.  #	You can only slice an index if the index is sorted (using .sort_index()).
            * sales.loc[‘shoes’ : ‘shorts’]       #To slice at the outer level, first and last can be strings. If you pass a single slice to .loc[], it will slice the rows
        * To slice inner index levels, you must pass the first and last index values as tuples. #To slice at inner levels, first and last should be tuples.
            * sales.loc[(‘shirt’, ‘red’) : (’shoes’, ‘red’)]
        * DataFrame columns can also be sliced. To do this, pass .loc[] two arguments: the rows to slice and the columns to slice, separated by a comma.
            * sales.loc[‘shirt’ : ‘shoes’, ‘profit’: ]
        * Slice twice: dogs_srt.loc[(“Labrador”, “Brown”):(“Schnauzer”, “Grey”), “name”:”height_cm”]
        * Dates are sliced in the same way as other data types, by specifying a starting and ending data, separated by a colon.
            * sales.loc[‘2021-04-28’:’2021-05-05’]
        * You can slice by partial dates, such as months ,without specifying the days.
            * sales.loc[‘2021-03’:’2021-04’]
        * You can also slice use .iloc[], like .loc[], iloc[] dose not include the ending values in the slice.
            * dogs.iloc[2:5, 1:4]

    3. working with pivot table 
        * You can subset pivot tables in the same way as DataFrames with sorted indexes. To slice rows by their index, call .loc[] on the pivot table, passing it the row indexes to include in the slice.
            * sales.loc[‘shirt’:’shoes’]
        * To slice rows and columns by their index, pass .loc[] the row and column indexes to include in the slice, separated by a comma.
            * sales.loc[’shirt’:’shoes’, ‘red’:’white’]
        * DataFrame summary statistic methods, such as .mean() and .sum() can also be used with pivot tables. Each method has an axis argument, which by default, is set to ‘index’. This means that summary statistics are calculated for each column by default.
            * sales.mean(axis=‘index’)
        * To calculate a summary for each row, set axis=columns
            * sales.mean(axis=‘columns’)

        * To access the components, subset the data column and follow it with .dt.component
            * dataframe["column"].dt.component
        * To access the year components, subset the data column and follow it with .dt.year
            * sales[‘data’].dt.year
        * To access the month components, subset the data column and follow it with .dt.month
            * sales[‘data’].dt.month
        * Example:
		# Pivot avg_temp_c by country and city vs year
		temp_by_country_city_vs_year = temperatures.pivot_table(values='avg_temp_c', index=(['country', 'city']), columns='year')
		# Subset in both directions at once
		print(temp_by_country_city_vs_year.loc[('Egypt','Cairo'):('India','Delhi'),'2005':'2010'])

   Chapter 4 : Creating and visualising Data
    1. Plotting 
        * Histogram 
            * Example:
		import matplotlib.pyplot as plt
		sat_scores[‘math’].hist(bins=20)
		plt.show()
        * Bar plot
            * Example :
		import matplotlib.pyplot as plt
		reading_by_borough = scores.groupby(‘Borough’)[‘reading’].mean
		reading_by_borough.plot(kind=‘bar’, title=‘Avg. Reading SATs by Borough’)
		plt.show()
        * Line plot
            * Example :
		import matplotlib.pyplot as plt
		# you can rotate the x-axis labels to improve readability by calling rot argument
		mens_100m_times_year.plot(x = ‘year’, y = ‘time_s’, kind = ‘line’, rot = 45)
		plt.show()
        * Scatter plots
            * Example:
		import matplotlib.pyplot as plt
		sat_scores.plot(x = ‘reading’, y = ‘writing’, kind=‘scatter’)
		plt.show()
        * Layering plots
            * Example:
		import matplotlib.pyplot as plt
		sat_scores[‘reading’].hist(alpha=0.8)
		sat_scores[‘writing’].hist(alpha=0.5)
		# a plt.legend() be called to identify the layered plots by color
		plt.legend([‘Avg. Reading SAT Score’, ‘Avg. Writing SAT Score’])
		plt.show()

    2. Handling missing data
        * NaN represent missing values and calling the .isna() method on a DataFrame produces a Boolean for every value depending on whether the value is missing or not
            * scores.isna().head()
        * Chaining .isna() with .any() produces a Boolean for each column, indicating whether it contains missing data or not
            * sat_scores.isna().any()
        * Changing .isna() with .sum() counts the number of True values in each column.
            * sat_scores.isna().sum()
        * Chaining .isna() with .sum and .plot(kind=‘bar’) allows you to visualise the number of missing values in each column
            * Example:
		import matplotlib.pyplot as plt
		sat_scores.isna().sum().plot(kind=‘bar’)
		plt.show()

        * .dropna() can remove any rows with missing values.
            * sat_scores.dropna().head()
        * .fillna() can replace NaN with another value.
            * sat_scores.fillna().head()

    3. Reading data into a DataFrame
        * Creating DataFrame from a list of dictionaries
            * Example 
		import pandas as pd
		list_of_dicts=[{k1:v1},{k2:v2},{k3:v3}]
		data_frame=pd.DataFrame(list_of_dicts)
        * Creating DataFrame from a list of dictionaries
            * Example 
		import pandas as pd
		dict_of_list={‘a’:[],’b’:[],’c’:[]}
		data_frame=pd.DataFrame(dict_of_list)
        * Creating DataFrame from a CSV file
            * Example 
		import pandas as pd
		data_frame=pd.read_csv(‘xxx.csv’)
        * Writing CSV file: calling .to_csv() method on a DataFrame and passing it a file path as a string
            * Example
		data_frame.to_csv(‘data_frame.csv’)


